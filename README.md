# Sylvach: Large-Scale Distributed Consensus Benchmark

Single-file benchmark comparing relaxed over-relaxation consensus against classic random pairwise gossip.

Sylvach leverages **first-principles linear algebra and contraction mappings** to accelerate convergence. It demonstrates iteration counts largely **independent of network size** for suitable Î±, while pairwise gossip scales as ~O(n log n) iterations.


## ðŸ”¹ Key Features

- **Dual Consensus Methods**
  - **Random Pairwise Gossip** â€” classic sparse, fully decentralized baseline (pairwise averaging)
  - **Sylvach (Relaxed Over-Relaxation)** â€” vectorized global-mean style updates with tunable relaxation parameter Î±  
    â†’ supports both over-relaxation (Î± > 1) and under-relaxation (Î± < 1)

- **Multi-State Support**
  - Scalar, vector, and symmetric matrix node states
  - Same core algorithm works for high-dimensional states (no code changes needed)

- **Large-Scale Capable**
  - Efficient NumPy vectorization
  - Comfortably handles **10k â€“ 500k+ nodes** in scalar mode
  - Still feasible memory & runtime even for large N

- **Î± Parameter Sweep**
  - Test multiple relaxation factors in one benchmark run
  - Explore trade-off between convergence speed and stability

- **Residual Tracking & Convergence Monitoring**
  - Per-iteration residual:  
    ```math
    \max_i \| \mathbf{x}_i - \mathbf{x}_\text{avg} \|
